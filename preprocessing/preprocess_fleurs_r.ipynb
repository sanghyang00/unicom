{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, OrderedDict\n",
    "import os, shutil, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "_FLEURS_LANG_TO_ID = OrderedDict([(\"Afrikaans\", \"af\"), (\"Amharic\", \"am\"), (\"Arabic\", \"ar\"), (\"Armenian\", \"hy\"), (\"Assamese\", \"as\"), (\"Asturian\", \"ast\"), (\"Azerbaijani\", \"az\"), (\"Belarusian\", \"be\"), (\"Bengali\", \"bn\"), (\"Bosnian\", \"bs\"), (\"Bulgarian\", \"bg\"), (\"Burmese\", \"my\"), (\"Catalan\", \"ca\"), (\"Cebuano\", \"ceb\"), (\"Mandarin Chinese\", \"cmn_hans\"), (\"Cantonese Chinese\", \"yue_hant\"), (\"Croatian\", \"hr\"), (\"Czech\", \"cs\"), (\"Danish\", \"da\"), (\"Dutch\", \"nl\"), (\"English\", \"en\"), (\"Estonian\", \"et\"), (\"Filipino\", \"fil\"), (\"Finnish\", \"fi\"), (\"French\", \"fr\"), (\"Fula\", \"ff\"), (\"Galician\", \"gl\"), (\"Ganda\", \"lg\"), (\"Georgian\", \"ka\"), (\"German\", \"de\"), (\"Greek\", \"el\"), (\"Gujarati\", \"gu\"), (\"Hausa\", \"ha\"), (\"Hebrew\", \"he\"), (\"Hindi\", \"hi\"), (\"Hungarian\", \"hu\"), (\"Icelandic\", \"is\"), (\"Igbo\", \"ig\"), (\"Indonesian\", \"id\"), (\"Irish\", \"ga\"), (\"Italian\", \"it\"), (\"Japanese\", \"ja\"), (\"Javanese\", \"jv\"), (\"Kabuverdianu\", \"kea\"), (\"Kamba\", \"kam\"), (\"Kannada\", \"kn\"), (\"Kazakh\", \"kk\"), (\"Khmer\", \"km\"), (\"Korean\", \"ko\"), (\"Kyrgyz\", \"ky\"), (\"Lao\", \"lo\"), (\"Latvian\", \"lv\"), (\"Lingala\", \"ln\"), (\"Lithuanian\", \"lt\"), (\"Luo\", \"luo\"), (\"Luxembourgish\", \"lb\"), (\"Macedonian\", \"mk\"), (\"Malay\", \"ms\"), (\"Malayalam\", \"ml\"), (\"Maltese\", \"mt\"), (\"Maori\", \"mi\"), (\"Marathi\", \"mr\"), (\"Mongolian\", \"mn\"), (\"Nepali\", \"ne\"), (\"Northern-Sotho\", \"nso\"), (\"Norwegian\", \"nb\"), (\"Nyanja\", \"ny\"), (\"Occitan\", \"oc\"), (\"Oriya\", \"or\"), (\"Oromo\", \"om\"), (\"Pashto\", \"ps\"), (\"Persian\", \"fa\"), (\"Polish\", \"pl\"), (\"Portuguese\", \"pt\"), (\"Punjabi\", \"pa\"), (\"Romanian\", \"ro\"), (\"Russian\", \"ru\"), (\"Serbian\", \"sr\"), (\"Shona\", \"sn\"), (\"Sindhi\", \"sd\"), (\"Slovak\", \"sk\"), (\"Slovenian\", \"sl\"), (\"Somali\", \"so\"), (\"Sorani-Kurdish\", \"ckb\"), (\"Spanish\", \"es\"), (\"Swahili\", \"sw\"), (\"Swedish\", \"sv\"), (\"Tajik\", \"tg\"), (\"Tamil\", \"ta\"), (\"Telugu\", \"te\"), (\"Thai\", \"th\"), (\"Turkish\", \"tr\"), (\"Ukrainian\", \"uk\"), (\"Umbundu\", \"umb\"), (\"Urdu\", \"ur\"), (\"Uzbek\", \"uz\"), (\"Vietnamese\", \"vi\"), (\"Welsh\", \"cy\"), (\"Wolof\", \"wo\"), (\"Xhosa\", \"xh\"), (\"Yoruba\", \"yo\"), (\"Zulu\", \"zu\")])\n",
    "_FLEURS_LANG_SHORT_TO_LONG = {v: k for k, v in _FLEURS_LANG_TO_ID.items()}\n",
    "\n",
    "_FLEURS_LANG = sorted([\"af_za\", \"am_et\", \"ar_eg\", \"as_in\", \"ast_es\", \"az_az\", \"be_by\", \"bn_in\", \"bs_ba\", \"ca_es\", \"ceb_ph\", \"cmn_hans_cn\", \"yue_hant_hk\", \"cs_cz\", \"cy_gb\", \"da_dk\", \"de_de\", \"el_gr\", \"en_us\", \"es_419\", \"et_ee\", \"fa_ir\", \"ff_sn\", \"fi_fi\", \"fil_ph\", \"fr_fr\", \"ga_ie\", \"gl_es\", \"gu_in\", \"ha_ng\", \"he_il\", \"hi_in\", \"hr_hr\", \"hu_hu\", \"hy_am\", \"id_id\", \"ig_ng\", \"is_is\", \"it_it\", \"ja_jp\", \"jv_id\", \"ka_ge\", \"kam_ke\", \"kea_cv\", \"kk_kz\", \"km_kh\", \"kn_in\", \"ko_kr\", \"ckb_iq\", \"ky_kg\", \"lb_lu\", \"lg_ug\", \"ln_cd\", \"lo_la\", \"lt_lt\", \"luo_ke\", \"lv_lv\", \"mi_nz\", \"mk_mk\", \"ml_in\", \"mn_mn\", \"mr_in\", \"ms_my\", \"mt_mt\", \"my_mm\", \"nb_no\", \"ne_np\", \"nl_nl\", \"nso_za\", \"ny_mw\", \"oc_fr\", \"om_et\", \"or_in\", \"pa_in\", \"pl_pl\", \"ps_af\", \"pt_br\", \"ro_ro\", \"ru_ru\", \"bg_bg\", \"sd_in\", \"sk_sk\", \"sl_si\", \"sn_zw\", \"so_so\", \"sr_rs\", \"sv_se\", \"sw_ke\", \"ta_in\", \"te_in\", \"tg_tj\", \"th_th\", \"tr_tr\", \"uk_ua\", \"umb_ao\", \"ur_pk\", \"uz_uz\", \"vi_vn\", \"wo_sn\", \"xh_za\", \"yo_ng\", \"zu_za\"])\n",
    "_FLEURS_LONG_TO_LANG = {_FLEURS_LANG_SHORT_TO_LONG[\"_\".join(k.split(\"_\")[:-1]) or k]: k for k in _FLEURS_LANG}\n",
    "_FLEURS_LANG_TO_LONG = {v: k for k, v in _FLEURS_LONG_TO_LANG.items()}\n",
    "\n",
    "_FLEURS_GROUP_TO_LONG = OrderedDict({\n",
    "    \"western_european_we\": [\"Asturian\", \"Bosnian\", \"Catalan\", \"Croatian\", \"Danish\", \"Dutch\", \"English\", \"Finnish\", \"French\", \"Galician\", \"German\", \"Greek\", \"Hungarian\", \"Icelandic\", \"Irish\", \"Italian\", \"Kabuverdianu\", \"Luxembourgish\", \"Maltese\", \"Norwegian\", \"Occitan\", \"Portuguese\", \"Spanish\", \"Swedish\", \"Welsh\"],\n",
    "    \"eastern_european_ee\": [\"Armenian\", \"Belarusian\", \"Bulgarian\", \"Czech\", \"Estonian\", \"Georgian\", \"Latvian\", \"Lithuanian\", \"Macedonian\", \"Polish\", \"Romanian\", \"Russian\", \"Serbian\", \"Slovak\", \"Slovenian\", \"Ukrainian\"],\n",
    "    \"central_asia_middle_north_african_cmn\": [\"Arabic\", \"Azerbaijani\", \"Hebrew\", \"Kazakh\", \"Kyrgyz\", \"Mongolian\", \"Pashto\", \"Persian\", \"Sorani-Kurdish\", \"Tajik\", \"Turkish\", \"Uzbek\"],\n",
    "    \"sub_saharan_african_ssa\": [\"Afrikaans\", \"Amharic\", \"Fula\", \"Ganda\", \"Hausa\", \"Igbo\", \"Kamba\", \"Lingala\", \"Luo\", \"Northern-Sotho\", \"Nyanja\", \"Oromo\", \"Shona\", \"Somali\", \"Swahili\", \"Umbundu\", \"Wolof\", \"Xhosa\", \"Yoruba\", \"Zulu\"],\n",
    "    \"south_asian_sa\": [\"Assamese\", \"Bengali\", \"Gujarati\", \"Hindi\", \"Kannada\", \"Malayalam\", \"Marathi\", \"Nepali\", \"Oriya\", \"Punjabi\", \"Sindhi\", \"Tamil\", \"Telugu\", \"Urdu\"],\n",
    "    \"south_east_asian_sea\": [\"Burmese\", \"Cebuano\", \"Filipino\", \"Indonesian\", \"Javanese\", \"Khmer\", \"Lao\", \"Malay\", \"Maori\", \"Thai\", \"Vietnamese\"],\n",
    "    \"chinese_japanase_korean_cjk\": [\"Mandarin Chinese\", \"Cantonese Chinese\", \"Japanese\", \"Korean\"],\n",
    "})\n",
    "_FLEURS_LONG_TO_GROUP = {a: k for k, v in _FLEURS_GROUP_TO_LONG.items() for a in v}\n",
    "_FLEURS_LANG_TO_GROUP = {_FLEURS_LONG_TO_LANG[k]: v for k, v in _FLEURS_LONG_TO_GROUP.items()}\n",
    "\n",
    "_ALL_LANG = _FLEURS_LANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_romanizer():\n",
    "    romanizer = ur.Uroman() # Usage: romanizer.romanize_string(text, lcode=iso)\n",
    "    \n",
    "    return romanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git lfs install\n",
    "# !git clone https://huggingface.co/datasets/google/fleurs-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleurs_dir = 'path/to/fleurs-r/dataset'\n",
    "fleurs_meta_dir = 'path/to/fleurs-r/dataset/metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'dev', 'test']\n",
    "columns = ['id', 'filename', 'raw transcription', 'transcription', 'character', 'sample length', 'gender']\n",
    "csvs = {\n",
    "    'train': [],\n",
    "    'dev': [],\n",
    "    'test': []\n",
    "}\n",
    "for lang_dir in tqdm(os.listdir(fleurs_meta_dir)):\n",
    "    for split in splits:\n",
    "        target_fname = os.path.join(fleurs_meta_dir, lang_dir, f'{split}.tsv')\n",
    "        # print(lang_dir, split)\n",
    "        data = pd.read_csv(target_fname, sep='\\t', names=columns, encoding='utf-8', quoting=csv.QUOTE_NONE)\n",
    "        if data.isna().any().any():\n",
    "            print(f'Error occured in {target_fname}')\n",
    "            nan_rows = data[data.isna().any(axis=1)]\n",
    "            nan_index = nan_rows.index\n",
    "            for index in nan_index:\n",
    "                # print(lang_dir, split, index)\n",
    "                t, c = data.loc[index, 'transcription'].strip().split('\\t')\n",
    "                t = t.strip()\n",
    "                c = c.strip()\n",
    "                # print(t, '\\n', c)\n",
    "                data.loc[index, 'gender'] = str(data.loc[index, 'sample length'])\n",
    "                data.loc[index, 'sample length'] = int(data.loc[index, 'character'])\n",
    "                data.loc[index, 'character'] = c\n",
    "                data.loc[index, 'transcription'] = t\n",
    "                # print('-------------------------------')\n",
    "                # print(data.loc[index, 'sample length'], type(data.loc[index, 'character']))\n",
    "                # print('-------------------------------')\n",
    "        length = len(data)\n",
    "        lang_ids = [_FLEURS_LANG.index(lang_dir) for i in range(length)]\n",
    "        langs = [_FLEURS_LANG_TO_LONG[lang_dir] for i in range(length)]\n",
    "        lang_groups = [_FLEURS_LANG_TO_GROUP[lang_dir] for i in range(length)]\n",
    "        data['duration'] = round(data['sample length'].astype(int) / SAMPLE_RATE, 2)\n",
    "        data['language id'] = lang_ids\n",
    "        data['language'] = langs\n",
    "        data['language group'] = lang_groups\n",
    "        csvs[split].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "for k, v in csvs.items():\n",
    "    # print(f'Split {k} have {len(v)} dataframes')\n",
    "    assert len(v)==102\n",
    "    \n",
    "    for df in v:\n",
    "        # print(df.isna().any(axis=1).sum())\n",
    "        assert not df.isna().any().any()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = {\n",
    "    'train': pd.DataFrame(),\n",
    "    'dev': pd.DataFrame(),\n",
    "    'test': pd.DataFrame()\n",
    "}\n",
    "for k, v in csvs.items():\n",
    "    merged[k] = pd.concat(v, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "for k, v in merged.items():\n",
    "    assert not v.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(fleurs_dir, 'csvs')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "for k, v in merged.items():\n",
    "    v.to_csv(os.path.join(save_path, f'{k}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(save_path, 'train.csv'))\n",
    "dev = pd.read_csv(os.path.join(save_path, 'dev.csv'))\n",
    "test = pd.read_csv(os.path.join(save_path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
